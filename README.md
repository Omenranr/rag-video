# Agentic AI Video Editing System
## Technical Presentation & Design Decisions

---

# 1: The Problem We're Solving

## The Challenge

**Traditional video editing is:**
- ‚è∞ Time-consuming (hours of manual work)
- üí∞ Expensive (requires skilled editors)
- üîÑ Repetitive (same tasks over and over)
- üö´ Not scalable (can't process thousands of videos)

**Example**: Blurring faces in a 10-minute video
- Manual editing: 2-3 hours
- Our system: 5-10 minutes

```mermaid
graph LR
    A[Manual Editing] -->|2-3 hours| B[Result]
    C[Our System] -->|5-10 minutes| D[Same Result]
    
    style A fill:#ff6b6b
    style C fill:#2ecc71
```

---

# 2: The Vision

## What If Video Editing Was As Simple As Talking?

**Instead of:**
```
1. Open video editor
2. Scrub through timeline
3. Manually select faces
4. Apply blur effect frame by frame
5. Export video
```

**Just say:**
```
"Blur all faces in this video"
```

### The Core Innovation
**Natural Language ‚Üí Automated Video Editing**

```mermaid
graph LR
    A[üë§ User] -->|Natural Language| B[ü§ñ AI Agent]
    B -->|Understands & Plans| C[üõ†Ô∏è Tools]
    C -->|Processes| D[üìπ Edited Video]
    
    style B fill:#4a90e2
    style C fill:#2ecc71
```

---

# 3: System Architecture - The Big Picture

## Why This Architecture?

**Decision**: Layered, modular architecture
**Reason**: Separation of concerns, scalability, maintainability

```mermaid
graph TB
    subgraph "Presentation Layer"
        UI[Next.js Frontend<br/>Why: Modern, fast, great UX]
    end
    
    subgraph "API Layer"
        API[FastAPI Gateway<br/>Why: Fast, async, auto-docs]
    end
    
    subgraph "Intelligence Layer"
        Agent[LangGraph Agent<br/>Why: Orchestrates complex workflows]
        Claude[Claude 3.5 Sonnet<br/>Why: Best reasoning capabilities]
    end
    
    subgraph "Tool Layer"
        YOLO[YOLO Server<br/>Why: Fast detection]
        SAM[SAM Server<br/>Why: Precise segmentation]
        Video[Video Server<br/>Why: Transformations]
    end
    
    subgraph "Data Layer"
        DB[(PostgreSQL<br/>Why: Reliable, ACID)]
        Storage[S3/MinIO<br/>Why: Scalable storage]
        Cache[(Redis<br/>Why: Fast caching)]
    end
    
    UI --> API
    API --> Agent
    Agent --> Claude
    Agent --> YOLO
    Agent --> SAM
    Agent --> Video
    YOLO --> DB
    SAM --> DB
    Video --> Storage
    API --> Cache
    
    style Agent fill:#4a90e2
    style Claude fill:#e24a90
    style YOLO fill:#2ecc71
    style SAM fill:#ff6b6b
```

---

# 4: The Two-Phase Approach

## Why Two Phases?

**Decision**: Separate analysis from processing
**Reason**: Speed, efficiency, cost optimization

### Phase 1: Cold Process (One-Time Analysis)
```mermaid
sequenceDiagram
    participant User
    participant System
    participant YOLO
    participant Database
    
    User->>System: Upload Video
    Note over System: Store video
    System->>YOLO: Analyze (background)
    Note over YOLO: Detect all objects<br/>in all frames
    YOLO->>Database: Store results
    Database->>User: ‚úÖ Ready for queries
    
    Note over User,Database: This happens ONCE per video<br/>Results reused for ALL queries
```

**Benefits:**
- ‚úÖ Analysis done once, used many times
- ‚úÖ User doesn't wait for analysis
- ‚úÖ Queries are instant (data already in DB)

### 2: Hot Process (Query Execution)
```mermaid
sequenceDiagram
    participant User
    participant Agent
    participant Database
    participant SAM
    participant Output
    
    User->>Agent: "Blur all faces"
    Agent->>Database: Get frames with persons
    Note over Database: Instant retrieval<br/>(already analyzed)
    Database->>Agent: Frame IDs + bboxes
    Agent->>SAM: Segment faces precisely
    SAM->>Agent: Pixel-perfect masks
    Agent->>Output: Apply blur + save
    Output->>User: Download edited video
    
    Note over User,Output: Fast because detection<br/>already done!
```

---

# 5: Why YOLOv8?

## The Detection Challenge

**Need**: Find objects in video frames quickly
**Solution**: YOLOv8 (You Only Look Once)

### Why YOLO?

```mermaid
graph TB
    subgraph "Requirements"
        R1[Fast<br/>Real-time processing]
        R2[Accurate<br/>High detection rate]
        R3[Versatile<br/>80 object classes]
        R4[Efficient<br/>Low resource usage]
    end
    
    subgraph "YOLO Advantages"
        A1[‚úÖ 100ms per frame GPU]
        A2[‚úÖ 53.9% mAP accuracy]
        A3[‚úÖ 80 COCO classes]
        A4[‚úÖ Only 6MB model size]
    end
    
    R1 --> A1
    R2 --> A2
    R3 --> A3
    R4 --> A4
    
    style R1 fill:#4a90e2
    style R2 fill:#4a90e2
    style R3 fill:#4a90e2
    style R4 fill:#4a90e2
    style A1 fill:#2ecc71
    style A2 fill:#2ecc71
    style A3 fill:#2ecc71
    style A4 fill:#2ecc71
```

### How YOLO Works

```mermaid
graph LR
    A[Input Frame<br/>640x640] --> B[Backbone<br/>Feature Extraction]
    B --> C[Neck<br/>Multi-scale Features]
    C --> D[Head<br/>Predictions]
    D --> E[Output<br/>Boxes + Classes]
    
    style B fill:#4a90e2
    style C fill:#2ecc71
    style D fill:#ff6b6b
```

**Key Decision**: Use batch processing (8 frames at once)
**Impact**: 2-3x faster than sequential processing

---

# 6: Why SAM3?

## The Precision Challenge

**Problem**: YOLO gives rectangles, we need exact shapes
**Solution**: SAM3 (Segment Anything Model)

### Why Not Just Use YOLO Boxes?

```mermaid
graph TB
    subgraph "YOLO Output"
        Y[Rectangle Box<br/>‚ùå Includes background<br/>‚ùå Not precise]
    end
    
    subgraph "SAM3 Output"
        S[Pixel-Perfect Mask<br/>‚úÖ Exact boundaries<br/>‚úÖ No background]
    end
    
    subgraph "Result Quality"
        R1[YOLO: Good enough<br/>for detection]
        R2[SAM3: Perfect<br/>for editing]
    end
    
    Y --> R1
    S --> R2
    
    style Y fill:#ff6b6b
    style S fill:#2ecc71
```

### SAM3 Architecture

```mermaid
graph TB
    A[Input Image] --> B[Image Encoder<br/>Vision Transformer]
    C[Prompts<br/>Bounding Boxes] --> D[Prompt Encoder]
    
    B --> E[Mask Decoder<br/>Combines Image + Prompt]
    D --> E
    
    E --> F[Segmentation Mask<br/>Pixel-Perfect]
    
    style B fill:#4a90e2
    style D fill:#2ecc71
    style E fill:#ff6b6b
    style F fill:#f39c12
```

**Key Decision**: Use YOLO boxes as prompts for SAM3
**Reason**: Best of both worlds - speed + precision

---

# 7: The YOLO + SAM Integration

## Why Combine Both Models?

**Decision**: Use YOLO for detection, SAM for segmentation
**Reason**: Each model excels at different tasks

```mermaid
graph LR
    A[Video Frame] --> B[YOLO Detection<br/>‚ö° Fast: 100ms<br/>üì¶ Output: Boxes]
    B --> C[SAM3 Segmentation<br/>üéØ Precise: 2-3s<br/>üé® Output: Masks]
    C --> D[Transformation<br/>‚ú® Apply Effects]
    D --> E[Final Result<br/>üé¨ Edited Frame]
    
    style B fill:#2ecc71
    style C fill:#4a90e2
    style D fill:#ff6b6b
    style E fill:#f39c12
```

### Performance Comparison

```mermaid
graph TB
    subgraph "Option 1: YOLO Only"
        O1[Fast but imprecise<br/>‚ö° 100ms per frame<br/>‚ùå Rectangle masks]
    end
    
    subgraph "Option 2: SAM Only"
        O2[Precise but slow<br/>üêå 5-10s per frame<br/>‚úÖ Perfect masks]
    end
    
    subgraph "Our Choice: YOLO + SAM"
        O3[Best of both<br/>‚ö° Fast detection<br/>‚úÖ Precise segmentation<br/>‚è±Ô∏è 2-3s total]
    end
    
    style O1 fill:#ff6b6b
    style O2 fill:#ff6b6b
    style O3 fill:#2ecc71
```

---

# 8: The Agentic Approach

## Why Use an AI Agent?

**Traditional Approach**: Hard-coded if/else logic
**Our Approach**: Intelligent agent that reasons

### The Problem with Traditional Code

```python
# Traditional approach - rigid and limited
if query == "blur faces":
    detect_persons()
    blur_faces()
elif query == "blur cars":
    detect_cars()
    blur_cars()
# What about "blur persons wearing red"? ü§î
```

### The Agentic Solution

```mermaid
graph TB
    A[User Query<br/>'Blur persons wearing red shirts'] --> B[Claude 3.5 Sonnet<br/>üß† Understands Intent]
    
    B --> C{Agent Reasoning}
    
    C --> D[Step 1: Detect 'person' class<br/>Tool: YOLO]
    C --> E[Step 2: Segment persons<br/>Tool: SAM3]
    C --> F[Step 3: Filter by 'red shirt'<br/>Tool: Vision LLM]
    C --> G[Step 4: Apply blur<br/>Tool: Video Transform]
    
    D --> H[Execute Plan]
    E --> H
    F --> H
    G --> H
    
    H --> I[‚úÖ Result]
    
    style B fill:#e24a90
    style C fill:#4a90e2
    style H fill:#2ecc71
```

**Key Benefit**: Handles complex, novel queries without new code

---

# 9: LangGraph State Machine

## Why LangGraph?

**Decision**: Use LangGraph for agent orchestration
**Reason**: Structured, debuggable, maintainable workflows

```mermaid
stateDiagram-v2
    [*] --> Understand
    
    Understand --> Plan
    note right of Understand
        Claude analyzes query
        Extracts: objects, actions, attributes
        Example: "blur" + "persons" + "red shirts"
    end note
    
    Plan --> Execute
    note right of Plan
        Creates step-by-step plan
        Selects appropriate tools
        Determines execution order
    end note
    
    Execute --> Execute: More steps?
    Execute --> Validate: All done
    note right of Execute
        Calls MCP tools sequentially
        YOLO ‚Üí SAM ‚Üí Transform
        Handles errors gracefully
    end note
    
    Validate --> Format
    note right of Validate
        Checks output quality
        Verifies file exists
        Validates processing
    end note
    
    Format --> [*]
    note right of Format
        Prepares response
        Generates download URL
        Returns metadata
    end note
```

**Benefits:**
- ‚úÖ Clear workflow visualization
- ‚úÖ Easy to debug and modify
- ‚úÖ Handles errors at each step
- ‚úÖ Maintains state across steps

---

# 10: MCP Protocol - The Tool Layer

## Why MCP (Model Context Protocol)?

**Decision**: Use MCP for tool integration
**Reason**: Standardized, modular, extensible

### The Problem Without MCP

```mermaid
graph TB
    A[Agent] --> B[YOLO Code]
    A --> C[SAM Code]
    A --> D[Video Code]
    A --> E[Future Tool 1]
    A --> F[Future Tool 2]
    
    style A fill:#ff6b6b
    note[Tightly coupled<br/>Hard to maintain<br/>Difficult to extend]
```

### The Solution With MCP

```mermaid
graph TB
    A[Agent] -->|MCP Protocol| B[MCP Server 1<br/>YOLO Tools]
    A -->|MCP Protocol| C[MCP Server 2<br/>SAM Tools]
    A -->|MCP Protocol| D[MCP Server 3<br/>Video Tools]
    A -->|MCP Protocol| E[Future Server<br/>New Tools]
    
    B --> F[YOLO Engine]
    C --> G[SAM Engine]
    D --> H[OpenCV/FFmpeg]
    E --> I[Any New Model]
    
    style A fill:#2ecc71
    style B fill:#4a90e2
    style C fill:#4a90e2
    style D fill:#4a90e2
    style E fill:#4a90e2
```

**Benefits:**
- ‚úÖ Loose coupling (easy to swap tools)
- ‚úÖ Standardized interface
- ‚úÖ Easy to add new tools
- ‚úÖ Independent scaling

---

# 11: MCP Server Architecture

## How MCP Servers Work

```mermaid
graph TB
    subgraph "Agent Layer"
        A[LangGraph Agent<br/>Needs to detect objects]
    end
    
    A -->|MCP Request| B
    
    subgraph "MCP Server"
        B[Protocol Handler]
        B --> C[Tools Registry]
        B --> D[Resources]
        B --> E[Prompts]
        
        C --> F[detect_objects]
        C --> G[get_frames_with_class]
        C --> H[get_available_classes]
    end
    
    F --> I[YOLO Engine]
    G --> I
    H --> I
    
    I -->|Results| B
    B -->|MCP Response| A
    
    style A fill:#4a90e2
    style B fill:#2ecc71
    style I fill:#ff6b6b
```

### Example: YOLO MCP Server Tools

```mermaid
graph LR
    A[Tool 1<br/>detect_objects_in_video] --> D[YOLO Engine]
    B[Tool 2<br/>get_frames_with_class] --> D
    C[Tool 3<br/>get_available_classes] --> D
    
    D --> E[Returns Results<br/>to Agent]
    
    style A fill:#4a90e2
    style B fill:#4a90e2
    style C fill:#4a90e2
    style D fill:#2ecc71
```

**Key Decision**: Each model gets its own MCP server
**Reason**: Independent deployment, scaling, and updates

---

# 12: The Complete Workflow

## From Query to Result

```mermaid
sequenceDiagram
    participant User
    participant Frontend
    participant API
    participant Agent
    participant Claude
    participant YOLO_MCP
    participant SAM_MCP
    participant Video_MCP
    participant Storage
    
    User->>Frontend: "Blur all faces"
    Frontend->>API: POST /api/query
    API->>Agent: Process query
    
    Agent->>Claude: Understand intent
    Note over Claude: Analyzes query<br/>Extracts: blur + faces
    Claude->>Agent: Intent + Plan
    
    Agent->>YOLO_MCP: get_frames_with_class("person")
    Note over YOLO_MCP: Queries database<br/>(already analyzed)
    YOLO_MCP->>Agent: Frames [10,25,40...] + boxes
    
    Agent->>SAM_MCP: segment_objects(frames, boxes, focus="head")
    Note over SAM_MCP: Precise segmentation<br/>of face regions
    SAM_MCP->>Agent: Pixel-perfect masks
    
    Agent->>Video_MCP: apply_blur(frames, masks, intensity=15)
    Note over Video_MCP: Applies Gaussian blur<br/>to masked regions
    Video_MCP->>Storage: Save edited video
    Video_MCP->>Agent: Video URL
    
    Agent->>API: Job complete
    API->>Frontend: Download URL
    Frontend->>User: ‚úÖ Video ready!
```

---

# 13: Graceful Degradation

## Why Design for Failure?

**Decision**: System works even without GPU
**Reason**: Reliability, accessibility, cost optimization

```mermaid
graph TB
    A[System Start] --> B{GPU Available?}
    
    B -->|Yes| C[Full Mode<br/>YOLO + SAM3]
    B -->|No| D[Degraded Mode<br/>YOLO + Bbox Masks]
    
    C --> E[High Quality<br/>Pixel-perfect masks<br/>2-3s per frame]
    D --> F[Good Quality<br/>Rectangle masks<br/>50ms per frame]
    
    E --> G[‚úÖ System Works]
    F --> G
    
    style C fill:#2ecc71
    style D fill:#f39c12
    style G fill:#4a90e2
```

### Performance Comparison

```mermaid
graph LR
    subgraph "GPU Mode Full"
        G1[YOLO: 100ms<br/>SAM3: 2-3s<br/>Total: ~3s/frame]
        G2[Quality: Excellent<br/>Precision: Pixel-perfect]
    end
    
    subgraph "CPU Mode Degraded"
        C1[YOLO: 500ms<br/>Bbox: 50ms<br/>Total: ~550ms/frame]
        C2[Quality: Good<br/>Precision: Rectangle]
    end
    
    G1 --> G2
    C1 --> C2
    
    style G1 fill:#2ecc71
    style G2 fill:#2ecc71
    style C1 fill:#f39c12
    style C2 fill:#f39c12
```

**Key Benefit**: System always functional, adapts to available resources

---

# 14: Database Design

## Why This Schema?

**Decision**: Separate tables for videos, detections, segmentations, jobs
**Reason**: Normalization, query efficiency, scalability

```mermaid
erDiagram
    VIDEOS ||--o{ DETECTIONS : "has many"
    VIDEOS ||--o{ SEGMENTATIONS : "has many"
    VIDEOS ||--o{ PROCESSING_JOBS : "has many"
    
    VIDEOS {
        uuid id PK
        string filename
        string storage_path
        float duration
        int fps
        timestamp uploaded_at
        string analysis_status
    }
    
    DETECTIONS {
        uuid id PK
        uuid video_id FK
        int frame_number
        string class_name
        float confidence
        jsonb bbox
        timestamp created_at
    }
    
    SEGMENTATIONS {
        uuid id PK
        uuid video_id FK
        int frame_number
        string object_class
        string mask_path
        int mask_area
        timestamp created_at
    }
    
    PROCESSING_JOBS {
        uuid id PK
        uuid video_id FK
        string query
        string status
        string result_path
        jsonb metadata
        timestamp completed_at
    }
```

### Why This Design?

```mermaid
graph TB
    A[Design Decision] --> B[Separate Detections Table]
    A --> C[JSONB for Bboxes]
    A --> D[Indexed Queries]
    
    B --> E[‚úÖ Reuse across queries<br/>‚úÖ Fast retrieval<br/>‚úÖ No reprocessing]
    C --> F[‚úÖ Flexible structure<br/>‚úÖ Easy to query<br/>‚úÖ PostgreSQL native]
    D --> G[‚úÖ Fast lookups<br/>‚úÖ Efficient filtering<br/>‚úÖ Scalable]
    
    style B fill:#4a90e2
    style C fill:#2ecc71
    style D fill:#ff6b6b
```

---

# 15: Performance Optimization

## How We Achieve Speed

### 1. Frame Sampling
```mermaid
graph LR
    A[All Frames<br/>1800 frames @ 30fps] --> B[Sample Every 5th<br/>360 frames]
    B --> C[5x Faster<br/>Same quality]
    
    style A fill:#ff6b6b
    style B fill:#2ecc71
    style C fill:#2ecc71
```

**Decision**: Process every 5th frame by default
**Reason**: 5x speed improvement, minimal quality loss

### 2. Batch Processing
```mermaid
graph TB
    subgraph "Sequential Processing"
        S1[Frame 1] --> S2[Frame 2] --> S3[Frame 3] --> S4[Frame 4]
        S5[Total: 400ms]
    end
    
    subgraph "Batch Processing"
        B1[Frames 1-4<br/>Processed Together]
        B2[Total: 150ms]
    end
    
    style S5 fill:#ff6b6b
    style B2 fill:#2ecc71
```

**Decision**: Process 8 frames per batch
**Impact**: 2-3x faster than sequential

### 3. Result Caching
```mermaid
graph LR
    A[Query 1<br/>Analyze video<br/>3 minutes] --> B[Store in DB]
    B --> C[Query 2<br/>Retrieve from DB<br/>100ms]
    
    style A fill:#ff6b6b
    style C fill:#2ecc71
```

**Decision**: Cache YOLO results in database
**Impact**: Instant retrieval for subsequent queries

---

# 16: Scaling Strategy

## üìà How We Scale

```mermaid
graph TB
    LB[Load Balancer<br/>Distributes requests]
    
    LB --> API1[FastAPI Instance 1]
    LB --> API2[FastAPI Instance 2]
    LB --> API3[FastAPI Instance 3]
    
    API1 --> Queue[Redis Queue<br/>Task distribution]
    API2 --> Queue
    API3 --> Queue
    
    Queue --> W1[Worker 1 + GPU<br/>Processes videos]
    Queue --> W2[Worker 2 + GPU<br/>Processes videos]
    Queue --> W3[Worker 3 + GPU<br/>Processes videos]
    
    W1 --> Storage[Shared Storage<br/>S3/MinIO]
    W2 --> Storage
    W3 --> Storage
    
    W1 --> DB[(Shared Database<br/>PostgreSQL)]
    W2 --> DB
    W3 --> DB
    
    style LB fill:#4a90e2
    style Queue fill:#2ecc71
    style Storage fill:#ff6b6b
    style DB fill:#f39c12
```

### Scaling Decisions

```mermaid
graph TB
    A[Scaling Decision] --> B[Horizontal Scaling]
    A --> C[Shared Storage]
    A --> D[Queue-based Processing]
    
    B --> E[‚úÖ Add more workers<br/>‚úÖ Linear scaling<br/>‚úÖ No single point of failure]
    C --> F[‚úÖ All workers access same data<br/>‚úÖ No data duplication<br/>‚úÖ Consistent state]
    D --> G[‚úÖ Async processing<br/>‚úÖ Load balancing<br/>‚úÖ Fault tolerance]
    
    style B fill:#4a90e2
    style C fill:#2ecc71
    style D fill:#ff6b6b
```

**Capacity**: Each worker processes ~10 videos/hour
**Scaling**: 3 workers = 30 videos/hour, 10 workers = 100 videos/hour

---

# 17: Real-World Use Cases

## Why This Matters

### Use Case 1: Privacy Protection
```mermaid
graph LR
    A[Social Media Video<br/>Contains faces] --> B[Our System<br/>'Blur all faces']
    B --> C[Privacy-Safe Video<br/>Ready to share]
    
    style A fill:#ff6b6b
    style B fill:#4a90e2
    style C fill:#2ecc71
```

**Impact**: GDPR compliance, protect identities, safe sharing

### Use Case 2: Content Moderation
```mermaid
graph LR
    A[User-Generated Content<br/>May contain sensitive info] --> B[Our System<br/>'Blur license plates']
    B --> C[Moderated Content<br/>Safe for platform]
    
    style A fill:#ff6b6b
    style B fill:#4a90e2
    style C fill:#2ecc71
```

**Impact**: Automated moderation, reduced manual review, faster processing

### Use Case 3: Security Footage
```mermaid
graph LR
    A[Security Camera<br/>24/7 recording] --> B[Our System<br/>'Blur faces except suspects']
    B --> C[Compliant Footage<br/>Protects innocent people]
    
    style A fill:#ff6b6b
    style B fill:#4a90e2
    style C fill:#2ecc71
```

**Impact**: Legal compliance, witness protection, selective anonymization

---

# 18: Competitive Advantages

## Benefits of this System?

```mermaid
graph TB
    subgraph "Our System"
        O1[Natural Language<br/>‚úÖ No training needed]
        O2[Agentic AI<br/>‚úÖ Handles novel queries]
        O3[Modular Design<br/>‚úÖ Easy to extend]
        O4[Production Ready<br/>‚úÖ Scalable, monitored]
        O5[Cost Effective<br/>‚úÖ Open source models]
    end
    
    subgraph "Manual Editing"
        M1[‚ùå Hours of work]
        M2[‚ùå Requires expertise]
        M3[‚ùå Not scalable]
        M4[‚ùå Expensive]
    end
    
    subgraph "Other AI Tools"
        A1[‚ö†Ô∏è Limited flexibility]
        A2[‚ö†Ô∏è Fixed workflows]
        A3[‚ö†Ô∏è Proprietary models]
        A4[‚ö†Ô∏è High costs]
    end
    
    style O1 fill:#2ecc71
    style O2 fill:#2ecc71
    style O3 fill:#2ecc71
    style O4 fill:#2ecc71
    style O5 fill:#2ecc71
    style M1 fill:#ff6b6b
    style M2 fill:#ff6b6b
    style M3 fill:#ff6b6b
    style M4 fill:#ff6b6b
    style A1 fill:#f39c12
    style A2 fill:#f39c12
    style A3 fill:#f39c12
    style A4 fill:#f39c12
```

---

# 19: Technical Specifications

## System Capabilities

### Processing Performance
```mermaid
graph LR
    subgraph "Input"
        I[1 minute video<br/>30 FPS<br/>1920x1080]
    end
    
    subgraph "Processing"
        P1[Cold Process<br/>YOLO Analysis<br/>~3 minutes]
        P2[Hot Process<br/>Query Execution<br/>~5-10 minutes]
    end
    
    subgraph "Output"
        O[Edited Video<br/>Same quality<br/>Same format]
    end
    
    I --> P1
    P1 --> P2
    P2 --> O
    
    style P1 fill:#4a90e2
    style P2 fill:#2ecc71
    style O fill:#f39c12
```

### Model Specifications
```mermaid
graph TB
    subgraph "YOLOv8"
        Y1[Size: 6MB]
        Y2[Speed: 100ms/frame]
        Y3[Classes: 80]
        Y4[Accuracy: 53.9% mAP]
    end
    
    subgraph "SAM3"
        S1[Size: 2.4GB]
        S2[Speed: 2-3s/frame]
        S3[Classes: Any]
        S4[Accuracy: IoU > 0.9]
    end
    
    subgraph "Claude 3.5"
        C1[Context: 200K tokens]
        C2[Reasoning: Excellent]
        C3[Speed: ~2s/query]
        C4[Cost: $3/$15 per 1M tokens]
    end
    
    style Y1 fill:#2ecc71
    style Y2 fill:#2ecc71
    style Y3 fill:#2ecc71
    style Y4 fill:#2ecc71
    style S1 fill:#4a90e2
    style S2 fill:#4a90e2
    style S3 fill:#4a90e2
    style S4 fill:#4a90e2
    style C1 fill:#e24a90
    style C2 fill:#e24a90
    style C3 fill:#e24a90
    style C4 fill:#e24a90
```

---

# 20: Future Roadmap

## What's Next?

```mermaid
graph TB
    A[Current System<br/>v1.0] --> B[Q1 2026]
    A --> C[Q2-Q3 2026]
    A --> D[Q4 2026+]
    
    B --> B1[Real-time Processing<br/>Live video support]
    B --> B2[Multi-object Tracking<br/>Track across frames]
    B --> B3[Scene Detection<br/>Automatic segmentation]
    
    C --> C1[CLIP Integration<br/>Better text filtering]
    C --> C2[Custom Fine-tuning<br/>Domain-specific models]
    C --> C3[Mobile Support<br/>iOS/Android apps]
    
    D --> D1[Edge Deployment<br/>On-device processing]
    D --> D2[Live Streaming<br/>Real-time editing]
    D --> D3[AR/VR Integration<br/>Immersive editing]
    
    style A fill:#4a90e2
    style B fill:#4a90e2
    style C fill:#4a90e2
    style D fill:#4a90e2
```

---
